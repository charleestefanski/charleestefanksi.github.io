<!DOCTYPE html>
<html>
<head>
    <title>Capstone Project: Optimizing Neural Information Retrieval Techniques</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
        .button {
            display: inline-block;
            padding: 10px 20px;
            text-align: center;
            text-decoration: none;
            color: #ffffff;
            background-color: #7aa8b7;
            border-radius: 6px;
            outline: none;
            transition: 0.3s;
        }
        .button:hover {
            background-color: #c2c7c7;
        }
    </style>
</head>
<body>

<!-- Navbar (sit on top) -->
<div class="w3-top">
    <div class="w3-bar w3-white w3-wide w3-padding w3-card">
        <a href="#home" class="w3-bar-item w3-button">Optimizing Neural Information Retrieval Techniques</a>
        <!-- Float links to the right. Hide them on small screens -->
        <div class="w3-right w3-hide-small">
            <a href="#project-overview" class="w3-bar-item w3-button">Project Overview</a>
            <a href="#data" class="w3-bar-item w3-button">Data</a>
            <a href="#technical-approaches" class="w3-bar-item w3-button">Technical Approach</a>
            <a href="#results" class="w3-bar-item w3-button">Results</a>
        </div>
    </div>
</div>

<!-- Header -->
<header class="w3-display-container w3-content" style="max-width:1500px;" id="home">
    <img class="w3-image" src="generic_ir.png" alt="Generic Information Retrieval" width="1500" height="800">
    <div class="w3-margin w3-display-left" style="top:100px;left:50px">
        <h2 class="w3-xlarge w3-text-white"><span class="w3-hide-small w3-text-dark-grey">UC Berkeley MIDS Capston Project, Spring 2023</span></h2>
    </div>
    <div class="w3-margin w3-display-left w3-padding-top-48" style="top:200px;left:50px">
        <h1 class="w3-xxxlarge w3-text-white w3-wide"><span class="w3-text-dark-grey">Optimizing Neural Information <br> Retrieval Techniques</span></h1>
        <a class="button w3-large" href="https://github.com/FirmwareDS/Capstone-Information-Retrieval-2.0"><i class="fa fa-github"></i> Project GitHub</a>
    </div>

    <div class="w3-row w3-half w3-margin w3-display-left" style="top:600px;left:50px">
        <h3 class="w3-xlarge w3-text-white w3-wide"><span class="w3-text-dark-grey">Team</span></h3>
        <div class = "w3-quarter">
            <h4>Marcus Manos</h4>
            <img src="marcus.png" alt="Marcus photo" style="width:90%">
        </div>
        <div class = "w3-quarter">
            <h4>Anil Tipirneni</h4>
            <img src="anil.JPG" alt="Anil photo" style="width:90%">
        </div>
        <div class = "w3-quarter">
            <h4>Charlee Stefanski</h4>
            <img src="charlee.jpg" alt="Charlee photo" style="width:90%">
        </div>
    </div>
</header>

<!-- Page content -->
<div class="w3-content w3-padding" style="max-width:1564px">

    <!-- Project Section -->
    <div class="w3-container w3-padding-32" id="project-overview">
        <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16 w3-text-dark-grey">Project Overview</h2>
    </div>

    <div class="w3-row">
        <div class="w3-half w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">Problem & Context</h3>
            <p>An information retrieval (IR) system is software which facilitates the organization, retrieval, and ranking or evaluation of relevant documents from a digital repository based on input from a user. People interact with IR systems constantly--to search for a song from a streaming service, find a restaurant in a new city, or find the receipt for an order in their email inbox. In a <a href = "https://ai.facebook.com/blog/-advances-toward-ubiquitous-neural-information-retrieval/">blog post</a> about neural information retrieval, Meta AI Research said IR is "arguably among the most defining challenges of the information age".</p>
            <p>In recent years, there has been an explosion of development in using deep neural networks and deep learning methods for IR tasks. Instead of relying on heavily pre-defined, handcrafted features for query-document matching and ranking, neural network models can learn relationships between words and phrases, patterns, and hierarchical structure. However, using deep learning comes with a high computational cost, and require large amounts of training data before they can be deployed.  </p>

        </div>
        <div class="w3-half w3-container w3-margin-bottom w3-padding-top-48">
            <img src="timeline.png" alt="Nueral IR Methods Timeline" style="width:99%">
            <figcaption>Figure from <a href = "https://europe.naverlabs.com/blog/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-first-stage-ranking/">Naver Labs Europe.</a></figcaption>
        </div>
    </div>

    <div class="w3-row w3-padding-top-48">
        <div class="w3-twothird w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">Goal</h3>
            <p>In this project, our task is to evaluate existing methods for IR, and attempt to narrow the efficiency-effectiveness gap. In other words, we want to develop a method which produces high-quality ranking results as efficiently as possible. </p>
            <p>To accomplish this task, we evaluated several recently published neural information retrieval methods and made improvements upon them.</p>
        </div>
    </div>

    <div class="w3-row">
        <div class="w3-half w3-container">
            <img src="generic_graph.png" alt="Graph of IR efficiency versus effectiveness" style="width:89%">
        </div>
        <div class="w3-half w3-container w3-sand">
            <h3 class="w3-large">How do we measure efficiency and effectiveness?</h3>
            <p>Efficiency:</p>
            <ul>
                <li>FLOPS (floating-point operations per second): Measure average FLOPS needed to compute document query score. Provides a metric of the computational and resource/energy demand of the model.</li>
                <li>Query Latency (in milliseconds): Time to return ranked documents for query.</li>
            </ul>
            <p>Effectiveness:</p>
            <ul>
                <li>Recall@K: Fraction of relevant docs of (K) items retrieved (order unaware).</li>
                <li>Mean Reciprocal Rank (MRR@K): Measure of ranking the first relevant document (order aware).</li>
            </ul>
        </div>
    </div>

    <!-- Data Section -->
    <div class="w3-container w3-padding-32" id="data">
        <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16 w3-text-dark-grey">Data: MS MARCO</h2>

        <div class="w3-row">
            <div class="w3-half w3-container w3-margin-bottom">
                <h3 class="w3-large w3-text-blue">About the Data</h3>
                <p>We used data from the <a href = "https://microsoft.github.io/msmarco/">MS MARCO collection of datasets</a> provided by Microsoft for our project.</p>
                <p>We use this dataset for several reasons...</p>
                <ul>
                    <li>The dataset contains 1,010,916 million anonymous queries/questions from Bing search engine, and 8,841,823 million passages from web documents retrieved by Bing. The collection of dataset includes pro-processed datasets for training and testing, as well as "triples" (query, positive passage, negative passage).</li>
                    <li>Using this dataset allows us to immediately start working on model development, and not worry about collecting, labeling and indexing data ourselves.</li>
                    <li>Almost all of the recent research on neural information retrieval utilizes MS MARCO and/or contains benchmarking results using MS MARCO. This enables us to more easily evaluate and compare our work with other research.</li>
                </ul>
            </div>
            <div class="w3-half w3-container w3-margin-top w3-padding-top-48">
                <img src="ms_marco_info.png" alt="Information about the MS MARCO data" style="width:99%">
            </div>
        </div>

        <div class="w3-row">
            <div class="w3-third w3-container w3-margin-bottom">
                <h3 class="w3-large w3-text-blue">Example Queries & Passages</h3>
                <p>Query: how many kiloliters are in a liter</p>
            </div>
            <div class="w3-half w3-container">
                <h3 class="w3-large w3-text-white">placeholder</h3>
                <p>Passage: How many kilojoules should an 18 year old girl have each day? You should be safe with around 8000-9000 kilojoules per day but it is highly relative to you personally and factors such as height, activity, age, gender and weight are inclus … ive in calculating how many kilojoules you should have.</p>
            </div>
        </div>

        <div class="w3-row">
            <div class="w3-third w3-container w3-margin-bottom">
                <p>Query: who were the maccabees</p>
            </div>
            <div class="w3-half w3-container w3-margin-bottom">
                <p>Passage 1: For other uses, see Maccabees (disambiguation). The Maccabees, also spelled Machabees (Hebrew: מכבים or מקבים‎‎, Maqabim; Latin: Machabaei or Maccabaei; Greek: μακκαβαῖοι, Makkabaioi), were the leaders of a Jewish rebel army that took control of Judea, which at the time had been a province of the Seleucid Empire.</p>
                <p>Passage 2: The descendants of Mattathias. The Maccabees, also spelled Machabees (Hebrew: מכבים or מקבים‎‎, Maqabim; Latin: Machabaei or Maccabaei; Greek: μακκαβαῖοι, Makkabaioi), were the leaders of a Jewish rebel army that took control of Judea, which at the time had been a province of the Seleucid Empire.</p>
            </div>
        </div>

        <div class="w3-row-padding">
            <div class="w3-col l3 m6 w3-margin-bottom">
                <div class="w3-content">
                    <img src="words_per_passage.png" alt="House" style="width:90%">
                    <figcaption>Distribution of the number of words (or length) of documents in the dataset.</figcaption>
                </div>
            </div>
            <div class="w3-col l3 m6 w3-margin-bottom">
                <div class="w3-content">
                    <img src="unique_words_per_passage.png" alt="House" style="width:100%">
                    <figcaption>Distribution of the number of unique words per document.</figcaption>
                </div>
            </div>
            <div class="w3-col l3 m6 w3-margin-bottom">
                <div class="w3-content">
<!--                    <div class="w3-display-topleft w3-black w3-padding">Fill</div>-->
                    <img src="query_top_words.png" alt="House" style="width:95%">
                    <figcaption>The most common non-stopwords in a query.</figcaption>
                </div>
            </div>
            <div class="w3-col l3 m6 w3-margin-bottom">
                <div class="w3-content">
<!--                    <div class="w3-display-topleft w3-black w3-padding">Fill</div>-->
                    <img src="query_top_ngrams.png" alt="House" style="width:100%">
                    <figcaption>The most common "tri-grams" of non-stopwords in a query.</figcaption>
                </div>
            </div>
        </div>

    </div>

    <!--Techincal Approach Section -->
    <div class="w3-container w3-padding-32" id="technical-approaches">
        <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16 w3-text-dark-grey">Technical Approach</h2>
    </div>

    <div class="w3-row">
        <div class="w3-twothird w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">ColBERT Overview</h3>
            <p><a href = "https://arxiv.org/pdf/2004.12832.pdf">ColBERT</a> is a ranking model which uses "contextualized late interation" with BERT. This method that leverages the effectiveness of deep learning models (specifically BERT) but is more efficient because it decreases the computational demand at the time of query processing using an architecture they call late interaction architecture.</p>
            <p>BERT, which was at one point the state-of-the-art neural information retrieval method as it offered more effective results than existing methods, uses "all-to-all interaction", where a neural network is fed inputs that reflect the similarity between each word and/or phrase in the query and documents. In order to speed up query processing, ColBERT encodes the query and document using BERT and then performs an interaction step where the similarity is computed. By delaying the interaction between queries and documents, document representation can be pre-computed offline. This makes the system much more efficient, but still offers the effectiveness of contextualized term embedding.</p>
        </div>
        <div class="w3-third w3-container w3-margin-top w3-padding-top-24">
            <img src="colbert_architecture.png" alt="House" style="width:100%">
            <figcaption>Figure from <a href = "https://arxiv.org/pdf/2004.12832.pdf">ColBERT Paper.</a></figcaption>
        </div>
    </div>

    <div class="w3-row">
        <div class="w3-third w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">Our Approach: ColBERT with Pruning</h3>
            <p>We attempted to improve upon ColBERT by creating pruning methods against ColBERT model layers.</p>
        </div>
        <div class="w3-third w3-container w3-margin-top w3-padding-top-24">
            <img src="pruning_mrr10.png" alt="House" style="width:100%">
        </div>
        <div class="w3-third w3-container w3-margin-top w3-padding-top-24">
            <img src="pruning_recall50.png" alt="House" style="width:100%">
        </div>
    </div>

    <div class="w3-row">
        <div class="w3-third w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">Our Approach: ColBERT with Quantization</h3>

            <p>One of the challenges of using neural networks for information retrieval is the high computational demand of training neural networks. We used a quantization technique to reduce the model size and therefore reduce the hardware demands of the model. </p>
            <p>Quantization of a model is the application of techniques for performing computations and storing tensors at lower bit-widths than floating point precision (i.e. using integers or smaller floating point values). This dramatically reduces both the memory requirement and computational cost of using neural networks. We applied dynamic quantization was post training and indexing.</p>
        </div>
        <div class="w3-twothird w3-container w3-margin-top w3-padding-top-24">
            <img src="quantization_example.png" alt="Quantization Code" style="width:100%">
        </div>
    </div>

    <div class="w3-row">
        <div class="w3-twothird w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">SPLADE Overview</h3>
            <p><a href = "https://arxiv.org/pdf/2107.05720.pdf">SPLADE</a> is a neural IR method which uses sparse representations of queries and documents. Utilizing sparse vectors means SPLADE still has the desirable properties of Bag-of-Words based methods (traditional methods), like interpretability, exact term matching, and efficient use of inverted indexes. Below is an example of the SPLADE representation of a document.</p>
            <table class="w3-table w3-border">
                <tr class="w3-blue">
                    <th>Query</th>
                    <th>Document</th>
                    <th>SPLADE BoW Representation</th>
                </tr>
                <tr>
                    <td>What is a corporation?</td>
                    <td>A corporation is a company or group of people authorized to act as a single entity and recognized as such in law.</td>
                    <td>[('corporation', 1.91), ('company', 1.29), ('single', 1.28), ('authorized', 1.21), ('entity', 1.01), ('group', 0.89), ('law', 0.82), ('recognized', 0.82), ('act', 0.8), ('a', 0.68), ('people', 0.44), ('as', 0.22)]</td>
                </tr>
            </table>
            <p>The architecture of SPLADE is illustrated in the figure on the right. A document/query is tokenized and then input to BERT. The output embeddings are then projected back to the full vocabulary. Then the signals are summed and a log-saturation effect is applied to term weights. This process results in a single representation is that is used to compute the document-query score. SPLADE is able to perform information retrieval more efficiently than many neural IR methods while maintaining similar effectiveness (MRR and recall).</p>
        </div>
        <div class="w3-third w3-container w3-margin-top w3-padding-top-48">
            <img src="splade_arch.png" alt="House" style="width:70%">
            <figcaption>Figure from <a href = "https://europe.naverlabs.com/blog/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-first-stage-ranking/">Naver Labs Europe.</a></figcaption>
        </div>
    </div>

    <div class="w3-row">
        <div class="w3-twothird w3-container w3-margin-bottom">
            <h3 class="w3-large w3-text-blue">Our Approach: SPLADE with SBERT</h3>

            <p>Our approach was to token-level transformer (BERT) with a sentence transformer (SBERT). This approach had a couple effects on the SPLADE model approach. First of all, the interpretability advantage of SPLADE was diminished because after using a sentence-level transformer we are no longer able to generate a token-level probability distribution to interpret why a document was included in the sparse index. Secondly, we hoped that the steps after the PLM would be more efficient since the model would not need to generate the probability distribution for each token (go through the hierarchical softmax and Concatenation step) and can instead it just produce the mean embeddings. However, this assumption turned out to be incorrect. </p>
        </div>
        <div class="w3-third w3-container w3-margin-top w3-padding-top-48">

        </div>
    </div>


    <!-- Results Section -->
    <div class="w3-container w3-padding-32" id="results">
        <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16 w3-text-dark-grey">Results</h2>

    </div>

    <div class="w3-container w3-margin-bottom">
        <h3 class="w3-large">ColBERT with Quantization</h3>

        <table class="w3-table w3-border w3-centered">
            <tr class="w3-blue-grey">
                <th>Dataset Size</th>
                <th>Model</th>
                <th>MRR@10</th>
                <th>Recall@50</th>
                <th>Recall@200</th>
                <th>Recall@1000</th>
                <th>Avg Encoding Time per Query (in ms)</th>
            </tr>
            <tr>
                <td>Toy dataset</td>
                <td>ColBERT</td>
                <td>0.395149</td>
                <td>0.866917</td>
                <td>0.945977</td>
                <td>0.976862</td>
                <td>6.59 ms</td>
            </tr>
            <tr>
                <td>Toy dataset</td>
                <td>ColBERT Quantized</td>
                <td>0.387623</td>
                <td>0.860005</td>
                <td>0.942574</td>
                <td>0.974558</td>
                <td>5.17 ms</td>
            </tr>
            <tr class="w3-khaki">
                <td></td>
                <td></td>
                <td>-0.01905</td>
                <td>-0.00797</td>
                <td>-0.0036</td>
                <td>-0.00236</td>
                <td>1.27x faster</td>
            </tr>
            <tr>
                <td>Full dataset</td>
                <td>ColBERT</td>
                <td>0.397918</td>
                <td>0.865142</td>
                <td>0.945777</td>
                <td>0.976369</td>
                <td>6.56 ms</td>
            </tr>
            <tr>
                <td>Full dataset</td>
                <td>ColBERT Quantized</td>
                <td>0.388929</td>
                <td>0.858006</td>
                <td>0.941289</td>
                <td>0.97341</td>
                <td>3.91 ms</td>
            </tr>
            <tr class="w3-khaki">
                <td></td>
                <td></td>
                <td>-0.02259</td>
                <td>-0.00825</td>
                <td>-0.00475</td>
                <td>-0.00303</td>
                <td>~1.7x faster</td>
            </tr>
        </table>
    </div>

    <div class="w3-container w3-margin-bottom">
        <h3 class="w3-large">SPLADE with SBERT</h3>

        <table class="w3-table w3-border w3-centered">
            <tr class="w3-blue-grey">
                <th>Model</th>
                <th>MRR@10</th>
                <th>FLOPS (Toy Dataset</th>
                <th>FLOPS (MS TREC 2019)</th>
                <th>Model Size (MB)</th>
            </tr>
            <tr>
                <td>SPLADE with Sentence Transformer</td>
                <td>0.33</td>
                <td>25,627</td>
                <td>27,688</td>
                <td>940</td>
            </tr>
            <tr>
                <td>SPLADE with Token-Level Transformer</td>
                <td>0.33</td>
                <td>6,458</td>
                <td>1,070</td>
                <td>767</td>
            </tr>
            <tr class="w3-khaki">
                <td>Percent Change</td>
                <td>0%</td>
                <td>-74%</td>
                <td>-96%</td>
                <td>-18%</td>
            </tr>
        </table>
    </div>

    <!-- End page content -->
</div>


<!-- Footer -->
<footer class="w3-center w3-black w3-padding-16">
    <p>Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-text-green">w3.css</a></p>
</footer>

</body>
</html>